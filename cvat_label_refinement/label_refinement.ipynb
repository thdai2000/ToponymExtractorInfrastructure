{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-04T12:19:40.012413200Z",
     "start_time": "2024-09-04T12:19:34.529981600Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import numpy as np\n",
    "from shapely.geometry import Polygon, LineString\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from PIL import Image, ImageDraw, ImageTk\n",
    "import tkinter as tk\n",
    "from tkinter import messagebox\n",
    "from tkinter import simpledialog\n",
    "import ast\n",
    "import json\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "font_path = r\"D:\\codes\\summer_intern_time_machine\\SegDetector\\dejavu-sans\\ttf\\DejaVuSans.ttf\"  # Ensure this path is correct\n",
    "font = ImageFont.truetype(font_path, 16)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-02T10:32:57.187040700Z",
     "start_time": "2024-09-02T10:32:57.159583300Z"
    }
   },
   "id": "c83927123edc9a52",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def find_file_recursive(directory, filename):\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        if filename in files:\n",
    "            return os.path.join(root, filename)\n",
    "    return None\n",
    "\n",
    "def is_clockwise(poly):\n",
    "    # Calculate the signed area using the shoelace formula\n",
    "    x, y = poly.exterior.coords.xy\n",
    "    area = 0.5 * sum(x[i] * y[i + 1] - x[i + 1] * y[i] for i in range(-1, len(x) - 1))\n",
    "    return area < 0\n",
    "\n",
    "def plot_polygons(original: list(), simplified: list()):\n",
    "    fig, ax = plt.subplots(dpi=300)\n",
    "    \n",
    "    # Draw original\n",
    "    for i, orig in enumerate(original):\n",
    "        x, y = orig.exterior.xy\n",
    "        if i == 0:\n",
    "            ax.plot(x, y, 'b-', label='Original Polygon', linewidth=1.3, alpha=0.7)\n",
    "        else:\n",
    "            ax.plot(x, y, 'b-', linewidth=1.3, alpha=0.7)\n",
    "\n",
    "    # Draw bezier polylines\n",
    "    for i, beziers in enumerate(simplified):\n",
    "        \n",
    "        for j, bezier in enumerate(beziers):\n",
    "            x, y = bezier.xy\n",
    "            if (i == 0) and (j == 0):\n",
    "                ax.plot(x, y, 'r-', label='Bezier curves', linewidth=1., alpha=0.6)\n",
    "            else:\n",
    "                ax.plot(x, y, 'r-', linewidth=1., alpha=0.6)\n",
    "\n",
    "    ax.set_title('Polygon Simplification')\n",
    "    ax.legend()\n",
    "    ax.set_aspect('equal')\n",
    "    plt.xlabel('X coordinate')\n",
    "    plt.ylabel('Y coordinate')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "def angle_between(v1, v2):\n",
    "    '''Computes the angle at a certain point on the polygon based on the two adjacent vertices'''\n",
    "    v1_u = v1 / np.linalg.norm(v1)\n",
    "    v2_u = v2 / np.linalg.norm(v2)\n",
    "    return np.arccos(np.clip(np.dot(v1_u, v2_u), -1.0, 1.0))\n",
    "\n",
    "def get_bezier_polylines(poly):\n",
    "    if not poly.is_valid or poly.is_empty:\n",
    "        return None\n",
    "    \n",
    "    coords = list(poly.exterior.coords)\n",
    "    coords = [coords[-2]] + coords\n",
    "    n = len(coords)\n",
    "    angles = []\n",
    "    \n",
    "    for i in range(1, n-1):\n",
    "        p0 = np.array(coords[i - 1])  # Last point\n",
    "        p1 = np.array(coords[i])      # This point\n",
    "        p2 = np.array(coords[(i + 1) % n])  # Next point\n",
    "        \n",
    "        v1 = p1 - p0  # Vect from the last point to the present one\n",
    "        v2 = p2 - p1  # Vect from the present point to the next one\n",
    "        \n",
    "        angle = angle_between(v1, v2)\n",
    "        angles.append(np.degrees(angle))\n",
    "\n",
    "    main_angles = sorted(np.argsort(angles)[-4:] + 1)\n",
    "    \n",
    "    l1 = LineString(coords[main_angles[0]:main_angles[1]+1])\n",
    "    l2 = LineString(coords[main_angles[1]:main_angles[2]+1])\n",
    "    l3 = LineString(coords[main_angles[2]:main_angles[3]+1])\n",
    "    l4 = LineString(coords[main_angles[3]:-1] + coords[1:main_angles[0]+1])\n",
    "\n",
    "    segments = np.array([l1, l2, l3, l4], dtype=object)\n",
    "    longest_segments_arg = np.argsort([l1.length, l2.length, l3.length, l4.length])[-2:]\n",
    "    beziers = segments[longest_segments_arg]\n",
    "\n",
    "    return beziers\n",
    "\n",
    "def get_bezier_polylines_single_word(poly):\n",
    "    if not poly.is_valid or poly.is_empty:\n",
    "        return None\n",
    "    \n",
    "    coords = list(poly.exterior.coords)\n",
    "    coords = [coords[-2]] + coords\n",
    "    n = len(coords)\n",
    "    angles = []\n",
    "    \n",
    "    for i in range(1, n-1):\n",
    "        p0 = np.array(coords[i - 1])  # Last point\n",
    "        p1 = np.array(coords[i])      # This point\n",
    "        p2 = np.array(coords[(i + 1) % n])  # Next point\n",
    "        \n",
    "        v1 = p1 - p0  # Vect from the last point to the present one\n",
    "        v2 = p2 - p1  # Vect from the present point to the next one\n",
    "        \n",
    "        angle = angle_between(v1, v2)\n",
    "        angles.append(np.degrees(angle))\n",
    "\n",
    "    main_angles = sorted(np.argsort(angles)[-4:] + 1)\n",
    "    \n",
    "    l1 = LineString(coords[main_angles[0]:main_angles[1]+1])\n",
    "    l2 = LineString(coords[main_angles[1]:main_angles[2]+1])\n",
    "    l3 = LineString(coords[main_angles[2]:main_angles[3]+1])\n",
    "    l4 = LineString(coords[main_angles[3]:-1] + coords[1:main_angles[0]+1])\n",
    "\n",
    "    segments = np.array([l1, l2, l3, l4], dtype=object)\n",
    "    \n",
    "    def compute_centroid(line):\n",
    "        coordinates = list(line.coords)\n",
    "        avg_x = sum(point[0] for point in coordinates) / len(coordinates)\n",
    "        avg_y = sum(point[1] for point in coordinates) / len(coordinates)\n",
    "        return (avg_x, avg_y)\n",
    "    \n",
    "    y1 = compute_centroid(l1)[1]\n",
    "    y2 = compute_centroid(l2)[1]\n",
    "    y3 = compute_centroid(l3)[1]\n",
    "    y4 = compute_centroid(l4)[1]\n",
    "    \n",
    "    # print(y1, y2, y3, y4)\n",
    "    \n",
    "    top_line_arg = np.argsort([y1, y2, y3, y4])[-1]\n",
    "    low_line_arg = np.argsort([y1, y2, y3, y4])[0]\n",
    "    \n",
    "    beziers = segments[[top_line_arg, low_line_arg]]\n",
    "    \n",
    "    # print(beziers)\n",
    "\n",
    "    return beziers\n",
    "\n",
    "\n",
    "def simplify_vertices(x):    \n",
    "    # try:\n",
    "    #     if len(x[\"text_cased\"]) <= 2 and x[\"group_id\"] == -1:\n",
    "    #         poly = Polygon(ori_vertices.copy() + [ori_vertices[0]]).buffer(0)\n",
    "    #         if not is_clockwise(poly):\n",
    "    #             poly = Polygon(np.flip(list(poly.exterior.coords), 0))\n",
    "    #         lines = get_bezier_polylines_single_word(poly)\n",
    "    #         new_vertices = [[(x, y) for x, y in lines[0].coords], [(x, y) for x, y in lines[1].coords]]\n",
    "    #         return new_vertices\n",
    "    # except:\n",
    "    #     pass\n",
    "    #     # print(x[\"text_cased\"])\n",
    "    #     # print(x[\"vertices\"])\n",
    "    try:\n",
    "        if len(x[\"text\"]) <= 2 and x[\"group_id\"] == -1:\n",
    "            try:\n",
    "                ori_vertices = x[\"vertices_corrected\"]\n",
    "                poly = Polygon(ori_vertices.copy() + [ori_vertices[0]]).buffer(0)\n",
    "                if not is_clockwise(poly):\n",
    "                    poly = Polygon(np.flip(list(poly.exterior.coords), 0))\n",
    "                lines = get_bezier_polylines_single_word(poly)\n",
    "                new_vertices = [[(x, y) for x, y in lines[0].coords], [(x, y) for x, y in lines[1].coords]]\n",
    "                return new_vertices\n",
    "            except:\n",
    "                ori_vertices = x[\"vertices\"]\n",
    "                poly = Polygon(ori_vertices.copy() + [ori_vertices[0]]).buffer(0)\n",
    "                if not is_clockwise(poly):\n",
    "                    poly = Polygon(np.flip(list(poly.exterior.coords), 0))\n",
    "                lines = get_bezier_polylines_single_word(poly)\n",
    "                new_vertices = [[(x, y) for x, y in lines[0].coords], [(x, y) for x, y in lines[1].coords]]\n",
    "                return new_vertices\n",
    "        else:\n",
    "            ori_vertices = x[\"vertices\"]\n",
    "            poly = Polygon(ori_vertices.copy() + [ori_vertices[0]]).buffer(0).simplify(3)\n",
    "            if not is_clockwise(poly):\n",
    "                poly = Polygon(np.flip(list(poly.exterior.coords), 0))\n",
    "            lines = get_bezier_polylines(poly)\n",
    "            new_vertices = [[(x, y) for x, y in lines[0].coords], [(x, y) for x, y in lines[1].coords]]\n",
    "            return new_vertices\n",
    "\n",
    "    except:\n",
    "        print(x[\"text\"])\n",
    "        return None\n",
    "    \n",
    "\n",
    "from shapely.geometry import Polygon\n",
    "from tqdm import tqdm\n",
    "\n",
    "def intersection_over_union(polygon1_points, polygon2_points):\n",
    "    \"\"\"\n",
    "    Calculate the Intersection over Union (IoU) of two polygons.\n",
    "\n",
    "    Parameters:\n",
    "    - polygon1_points: List of (x, y) tuples representing the first polygon.\n",
    "    - polygon2_points: List of (x, y) tuples representing the second polygon.\n",
    "\n",
    "    Returns:\n",
    "    - iou: Intersection over Union metric.\n",
    "    \"\"\"\n",
    "    poly1 = Polygon(polygon1_points)\n",
    "    poly2 = Polygon(polygon2_points)\n",
    "    \n",
    "    if not poly1.is_valid or not poly2.is_valid:\n",
    "        return 0\n",
    "\n",
    "    intersection_area = poly1.intersection(poly2).area\n",
    "    union_area = poly1.union(poly2).area\n",
    "    \n",
    "    iou = intersection_area / union_area if union_area != 0 else 0\n",
    "    \n",
    "    return iou\n",
    "\n",
    "\n",
    "def match_polygons(polygon_set1, polygon_set2, iou_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Match polygons from two sets based on IoU.\n",
    "\n",
    "    Parameters:\n",
    "    - polygon_set1: List of polygons (each polygon is a list of (x, y) tuples) representing the first set.\n",
    "    - polygon_set2: List of polygons (each polygon is a list of (x, y) tuples) representing the second set.\n",
    "    - iou_threshold: Threshold for considering two polygons as a match.\n",
    "\n",
    "    Returns:\n",
    "    - matches: List of tuples, each containing the indices of matched polygons (index in set1, index in set2).\n",
    "    \"\"\"\n",
    "    matches = []\n",
    "    used_indices = set()\n",
    "\n",
    "    for i, poly1 in tqdm(enumerate(polygon_set1)):\n",
    "        best_iou = 0  # best intersection over union\n",
    "        best_match_index = -1\n",
    "        for j, poly2 in enumerate(polygon_set2):\n",
    "            if j in used_indices:\n",
    "                continue\n",
    "            iou = intersection_over_union(poly1, poly2)\n",
    "            if iou > best_iou:\n",
    "                best_iou = iou\n",
    "                best_match_index = j\n",
    "        \n",
    "        if best_iou >= iou_threshold and best_match_index != -1:\n",
    "            matches.append((i, best_match_index))\n",
    "            used_indices.add(best_match_index)\n",
    "    \n",
    "    return matches\n",
    "\n",
    "\n",
    "def plot_matches(img_name, annotation_polygons, model_polygons, matches):\n",
    "    \n",
    "    file_path = find_file_recursive(\"./new_annotation/maps_res/\", img_name)\n",
    "    dir_path = os.path.dirname(file_path)\n",
    "    \n",
    "    image = Image.open(file_path)\n",
    "    \n",
    "    draw = ImageDraw.Draw(image)\n",
    "    \n",
    "    matched_annot_id = [match[0] for match in matches]\n",
    "    matched_model_id = [match[1] for match in matches]\n",
    "    unmatched_annot_id = [i for i in range(len(annotation_polygons)) if i not in matched_annot_id]\n",
    "    unmatched_model_id = [i for i in range(len(model_polygons)) if i not in matched_model_id]\n",
    "    \n",
    "    for match in matches:\n",
    "        color = tuple(np.random.choice(range(256), size=3))\n",
    "        draw.line(annotation_polygons[match[0]], fill=color, width=2)\n",
    "        draw.line(model_polygons[match[1]], fill=color, width=2)\n",
    "        draw.text(model_polygons[match[1]][-1], text=df_model.iloc[match[1]].text, font=font)\n",
    "    \n",
    "    for unmatched_annot in unmatched_annot_id:\n",
    "        draw.line(annotation_polygons[unmatched_annot], fill=\"black\", width=2)\n",
    "    \n",
    "    for unmatched_model in unmatched_model_id:\n",
    "        draw.line(model_polygons[unmatched_model], fill=\"white\", width=2)\n",
    "    \n",
    "    # image.show()\n",
    "    image.save(dir_path + \"/\" + img_name.split('.')[0]+\"_matching_res.png\")\n",
    "    \n",
    "\n",
    "def plot_propagation(img_name, df_merged):\n",
    "    \n",
    "    file_path = find_file_recursive(\"./new_annotation/maps_res/\", img_name)\n",
    "    dir_path = os.path.dirname(file_path)\n",
    "    \n",
    "    image = Image.open(file_path)\n",
    "\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    \n",
    "    df_merged_valid = df_merged.dropna(subset=['vertices_corrected'])\n",
    "    for polygon, text in zip(df_merged_valid['vertices_corrected'].tolist(), df_merged_valid['text'].tolist()):\n",
    "        draw.line(polygon, fill='green', width=2)\n",
    "        draw.text(polygon[-1], text, fill=\"green\", font=font)\n",
    "    \n",
    "    \n",
    "    df_merged_na = df_merged[df_merged['vertices_corrected'].isna()]\n",
    "    for polygon, text in zip(df_merged_na['vertices'].tolist(), df_merged_na['text'].tolist()):\n",
    "        draw.line(polygon, fill='red', width=2)\n",
    "        draw.text(polygon[0], text, fill=\"red\", font=font)\n",
    "    \n",
    "    # image.show()\n",
    "    image.save(dir_path + \"/\" + img_name.split('.')[0]+\"_propagated_res.png\")\n",
    "    \n",
    "\n",
    "def changecase(word):\n",
    "    ocr_text = str(word[\"OCR_text\"])\n",
    "    annotated_text = str(word[\"text\"])\n",
    "    returned_str = annotated_text\n",
    "            \n",
    "    for i, char in enumerate(ocr_text):\n",
    "        if char.isupper():\n",
    "            returned_str = annotated_text[:i] + annotated_text[i:].capitalize()\n",
    "            try:\n",
    "                if ocr_text[i+1].isupper():\n",
    "                    returned_str = annotated_text.upper()\n",
    "                    break\n",
    "            except IndexError:\n",
    "                break\n",
    "    \n",
    "    return returned_str\n",
    "\n",
    "\n",
    "def get_features(x):\n",
    "    try:\n",
    "        line1 = x['vertices_simplified'][0]\n",
    "        line2 = x['vertices_simplified'][1]\n",
    "        feature = [line2[0][0] - line1[0][0], # x of first point of upper to the first point of lower\n",
    "               line2[0][1] - line1[0][1], # y of first point of upper to the first point of lower\n",
    "               line2[-1][0] - line1[-1][0], # y of first point of upper to the first point of lower\n",
    "               line2[-1][1] - line1[-1][1], # y of first point of upper to the first point of lower\n",
    "               ]\n",
    "        return feature\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "\n",
    "def correct_order(x, model):\n",
    "    try:\n",
    "        line1 = x[\"vertices_simplified\"][0]\n",
    "        line2 = x[\"vertices_simplified\"][1]\n",
    "        y_pred = model.predict([x[\"feature\"]]).item()\n",
    "        if y_pred == 1:\n",
    "            return [line1, line2]\n",
    "        else:\n",
    "            return [line2, line1]\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "    \n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
    "\n",
    "def get_textual_direction(vertices):\n",
    "    \n",
    "    diff = vertices[0][-1] - vertices[0][0]\n",
    "    \n",
    "    return diff\n",
    "\n",
    "\n",
    "def normalize(vectors):\n",
    "    return vectors / np.linalg.norm(vectors, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "def cosine_similarity_matrix(vectors):\n",
    "    normalized_vectors = normalize(vectors)\n",
    "    return np.dot(normalized_vectors, normalized_vectors.T)\n",
    "\n",
    "\n",
    "def group_vectors(vectors):\n",
    "    similarity_matrix = cosine_similarity_matrix(vectors)\n",
    "    if np.all(similarity_matrix > 0):\n",
    "        # All vectors have positive cosine similarity, they are in one group\n",
    "        return np.zeros(len(vectors), dtype=int)\n",
    "    else:\n",
    "        # Some vectors have negative cosine similarity, we need to split into two groups\n",
    "        distance_matrix = 1 - similarity_matrix\n",
    "        clustering = AgglomerativeClustering(n_clusters=2, affinity='precomputed', linkage='complete')\n",
    "        # clustering = KMeans(n_clusters=2)\n",
    "        labels = clustering.fit_predict(distance_matrix)\n",
    "        return labels\n",
    "\n",
    "\n",
    "def unify_group_direction(x):\n",
    "    \n",
    "    crop_name, group_id = x.name\n",
    "    if group_id == -1:\n",
    "        return x\n",
    "    \n",
    "    polylines = x[\"vertices_simplified_order_corrected\"].tolist()\n",
    "    vectors = []\n",
    "    try:\n",
    "        for lines in polylines:\n",
    "            vectors.append((np.array(lines[0][-1]) - np.array(lines[0][0])).tolist())\n",
    "    except:\n",
    "        # print(polylines)\n",
    "        return x\n",
    "    \n",
    "    labels = group_vectors(np.array(vectors))\n",
    "    # print(labels)\n",
    "    \n",
    "    if 1 not in labels:\n",
    "        return x\n",
    "    else:\n",
    "        # print(labels)\n",
    "        num_ones = np.sum(labels)\n",
    "        num_zeros = len(labels) - num_ones\n",
    "        if num_ones > num_zeros:\n",
    "            for i in np.where(labels==0)[0].tolist():\n",
    "                polylines[i] = [polylines[i][1], polylines[i][0]]\n",
    "        elif num_ones < num_zeros:\n",
    "            for i in np.where(labels==1)[0].tolist():\n",
    "                polylines[i] = [polylines[i][1], polylines[i][0]]\n",
    "        new_x = x.copy()\n",
    "        new_x[\"vertices_simplified_order_corrected\"] = polylines\n",
    "        \n",
    "        return new_x\n",
    "    \n",
    "\n",
    "def calculate_polyline_centroid(lines):\n",
    "    \n",
    "    centroid_x = (np.mean(np.array(lines[0])[:, 0]) + np.mean(np.array(lines[1])[:, 0])) / 2\n",
    "    centroid_y = (np.mean(np.array(lines[0])[:, 1]) + np.mean(np.array(lines[1])[:, 1])) / 2\n",
    "    \n",
    "    return np.array([centroid_x, centroid_y])\n",
    "\n",
    "\n",
    "def identify_group_direction(x):\n",
    "    \n",
    "    polylines = x[\"vertices_simplified_order_corrected\"].tolist()\n",
    "    try:\n",
    "        first_text_direction = normalize([np.array(polylines[0][0][-1]) - np.array(polylines[0][0][0])])\n",
    "        \n",
    "        centroid_second = calculate_polyline_centroid(polylines[1])\n",
    "        centroid_first = calculate_polyline_centroid(polylines[0])\n",
    "    \n",
    "        base_direction = normalize([np.array(centroid_second) - np.array(centroid_first)])\n",
    "        angle = np.arccos(np.clip(np.dot(base_direction[0], first_text_direction[0]), -1.0, 1.0))\n",
    "        if angle > (3/4) * np.pi:\n",
    "            # print(base_direction[0], first_text_direction[0])\n",
    "            for i, lines in enumerate(polylines):\n",
    "                polylines[i] = [lines[1], lines[0]]\n",
    "            new_x = x.copy()\n",
    "            new_x[\"vertices_simplified_order_corrected\"] = polylines\n",
    "            \n",
    "            return new_x\n",
    "        else:\n",
    "            return x\n",
    "    except:\n",
    "        return x\n",
    "    \n",
    "\n",
    "def plot_final(img_name, df):\n",
    "    \n",
    "    file_path = find_file_recursive(\"./new_annotation/maps_res/\", img_name)\n",
    "    dir_path = os.path.dirname(file_path)\n",
    "    \n",
    "    image = Image.open(file_path)\n",
    "    \n",
    "    draw = ImageDraw.Draw(image)\n",
    "    \n",
    "    corrected_vertices = df[df[\"crop_name\"]==img_name][\"vertices_simplified_order_corrected\"].tolist()\n",
    "    original_vertices = df[df[\"crop_name\"]==img_name][\"vertices\"].tolist()\n",
    "    for i, vertices in enumerate(corrected_vertices):\n",
    "        if vertices:\n",
    "            draw.line(vertices[0] + vertices[1], fill='orange', width=2)\n",
    "            draw.line(vertices[0], fill=\"blue\", width=2)\n",
    "            draw.line(vertices[1], fill=\"blue\", width=2)\n",
    "        else:\n",
    "            vertices = original_vertices[i]\n",
    "            draw.line(vertices, fill='red', width=2)\n",
    "        \n",
    "    # image.show()\n",
    "    image.save(dir_path + \"/\" + img_name.split('.')[0]+\"_final_corr.png\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-04T12:19:46.077440900Z",
     "start_time": "2024-09-04T12:19:45.002986400Z"
    }
   },
   "id": "d4365617a147c08b",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_annotation = pd.read_pickle(\"./new_annotation/final/cvat_annotations_20240806_pandas.pkl\")\n",
    "df_annotation['vertices'] = df_annotation['vertices'].apply(lambda x: [(point[0], point[1]) for point in x])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-19T14:02:04.947084700Z",
     "start_time": "2024-08-19T14:02:02.684072800Z"
    }
   },
   "id": "65dd28bfbda6b075",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_annotation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m crop \u001B[38;5;129;01min\u001B[39;00m \u001B[43mdf_annotation\u001B[49m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcrop_name\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39munique()\u001B[38;5;241m.\u001B[39mtolist():\n\u001B[0;32m      3\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m      4\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mProcessing \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(crop))\n",
      "\u001B[1;31mNameError\u001B[0m: name 'df_annotation' is not defined"
     ]
    }
   ],
   "source": [
    "for crop in df_annotation[\"crop_name\"].unique().tolist():\n",
    "    \n",
    "    try:\n",
    "        print(\"Processing {}\".format(crop))\n",
    "    \n",
    "        df = df_annotation[df_annotation[\"crop_name\"] == crop]\n",
    "        file_path = find_file_recursive(\"./new_annotation/maps_res/\", crop)\n",
    "        dir_path = os.path.dirname(file_path)\n",
    "        df_model = pd.read_json(dir_path + \"/flattened_detections.json\")\n",
    "        \n",
    "        annotation_polygons = df['vertices'].tolist()\n",
    "        model_polygons = df_model.apply(lambda x: [(x.polygon_x[i], x.polygon_y[i]) for i in range(len(x.polygon_x))], axis=1).tolist()\n",
    "        \n",
    "        iou_threshold = 0.6\n",
    "        matches = match_polygons(annotation_polygons, model_polygons, iou_threshold)\n",
    "        \n",
    "        # plot_matches(crop, annotation_polygons, model_polygons, matches)\n",
    "        \n",
    "        df_matches = None\n",
    "        for match in matches:\n",
    "            df_matches = pd.concat((df_matches, pd.DataFrame({\"vertices\": [df.iloc[match[0]].vertices],\n",
    "                                                             \"vertices_corrected\": [model_polygons[match[1]]],\n",
    "                                                              \"OCR_text\": [df_model.iloc[match[1]][\"text\"]]})))\n",
    "            \n",
    "        df['vertices'] = df['vertices'].apply(str)\n",
    "        df_matches['vertices'] = df_matches['vertices'].apply(str)\n",
    "        df_merged = pd.merge(df, df_matches, on='vertices', how='outer')\n",
    "        df_merged['vertices'] = df_merged['vertices'].apply(ast.literal_eval)\n",
    "        df_merged['vertices'] = df_merged['vertices'].apply(lambda x: [(point[0], point[1]) for point in x])\n",
    "        \n",
    "        df_merged['text'] = df_merged.apply(changecase, axis=1)\n",
    "        \n",
    "        # plot_propagation(crop, df_merged)\n",
    "        \n",
    "        df_merged[\"vertices_simplified\"] = df_merged.apply(simplify_vertices, axis=1)\n",
    "        \n",
    "        df_merged[\"feature\"] = df_merged.apply(get_features, axis=1)\n",
    "        \n",
    "        with open(\"ord_cor.pkl\", \"rb\") as f:\n",
    "            model = pickle.load(f)\n",
    "            \n",
    "        df_merged[\"vertices_simplified_order_corrected\"] = df_merged.apply(correct_order, args=(model,), axis=1)\n",
    "        \n",
    "        # step1: unify the direction based on majority vote\n",
    "        df_merged_modified = df_merged.groupby([\"crop_name\", \"group_id\"]).apply(unify_group_direction).reset_index(drop=True)\n",
    "        \n",
    "        # step2: identify the direction based on base direction\n",
    "        grouped = df_merged_modified.groupby(['crop_name', 'group_id'])\n",
    "        modified_groups = []\n",
    "        for (crop_name, group_id), group in grouped:\n",
    "            # Sort each group by sequence_id before applying the custom function\n",
    "            if group_id == -1:\n",
    "                modified_groups.append(group)\n",
    "                continue\n",
    "            group = group.sort_values(by='sequence_order')\n",
    "            modified_group = identify_group_direction(group)\n",
    "            modified_groups.append(modified_group)\n",
    "        df_merged_identified = pd.concat(modified_groups).reset_index(drop=True)\n",
    "        \n",
    "        plot_final(crop, df_merged_identified)\n",
    "        \n",
    "        df_merged_identified.to_pickle(dir_path + \"/\" + crop + \".pkl\")\n",
    "    \n",
    "    except:\n",
    "        continue"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-04T12:20:10.374566Z",
     "start_time": "2024-09-04T12:20:07.928568900Z"
    }
   },
   "id": "7c07480482716723",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "LogisticRegression()",
      "text/html": "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"annotations.json\", 'r') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "# Extract features and labels\n",
    "features = [sample[\"features\"] for sample in data]\n",
    "labels = [sample[\"label\"] for sample in data]\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X = np.array(features)\n",
    "y = np.array(labels)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "with open(\"ord_cor.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-13T19:45:35.752098500Z",
     "start_time": "2024-08-13T19:45:34.320100400Z"
    }
   },
   "id": "16e1760c44697ddd",
   "execution_count": 73
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "for root, dirs, files in os.walk(\"./new_annotation/maps_res\"):\n",
    "    for file in files:\n",
    "        if \"final_corr\" in file:\n",
    "            shutil.copy(os.path.join(root, file), os.path.join(\"new_annotation/vis\", file))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-19T14:58:43.390967100Z",
     "start_time": "2024-08-19T14:58:42.789967600Z"
    }
   },
   "id": "ee24f3ae52b7026d",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def points_to_str(pts):\n",
    "    \n",
    "    pts_str = []\n",
    "    upper, lower = pts[0], pts[1]\n",
    "    for pt in lower[::-1]:\n",
    "        pt_str = str(pt[0]) + \",\" + str(pt[1])\n",
    "        pts_str.append(pt_str)\n",
    "    for pt in upper[::-1]:\n",
    "        pt_str = str(pt[0]) + \",\" + str(pt[1])\n",
    "        pts_str.append(pt_str)\n",
    "    return \";\".join(pts_str)\n",
    "\n",
    "    \n",
    "def get_image_dimensions(image_path):\n",
    "    with Image.open(image_path) as img:\n",
    "        width, height = img.size\n",
    "    return width, height\n",
    "\n",
    "\n",
    "def get_annotation_dicts_from_df(df):\n",
    "    \n",
    "    annotations = []\n",
    "    \n",
    "    for row in df.iterrows():\n",
    "        if row[1][-1]:\n",
    "            points = row[1][-1]\n",
    "            points_str = points_to_str(points)\n",
    "        else:\n",
    "            points = row[1][0]\n",
    "            points_str = \";\".join([str(pt[0]) + \",\" + str(pt[1]) for pt in points])\n",
    "        group_id = row[1][4]\n",
    "        sequence_order = row[1][5]\n",
    "        if group_id == -1:\n",
    "            group_id = 1\n",
    "            sequence_order = None\n",
    "        else:\n",
    "            group_id = group_id\n",
    "            sequence_order = sequence_order\n",
    "        transcription = row[1][1]\n",
    "        truncated = row[1][3]\n",
    "        illegible = row[1][2]\n",
    "        \n",
    "        word_dict = {\"transcription\": transcription,\n",
    "                \"points\": points_str,\n",
    "                \"group_id\": group_id,\n",
    "                \"sequence_order\": sequence_order,\n",
    "                \"truncated\": truncated,\n",
    "                \"illegible\": illegible,\n",
    "                \"z_order\": -1}\n",
    "        \n",
    "        annotations.append(word_dict)\n",
    "    \n",
    "    return annotations\n",
    "\n",
    "\n",
    "def convert_pkl_to_json(df):\n",
    "    \n",
    "    img_name = df[\"crop_name\"].tolist()[0]\n",
    "    img_path = find_file_recursive(\"./new_annotation/maps_res\", img_name)\n",
    "    width, height = get_image_dimensions(img_path)\n",
    "    annotations = get_annotation_dicts_from_df(df)\n",
    "    \n",
    "    img_annotation = {\"image_name\": img_name,\n",
    "                      \"width\": width,\n",
    "                      \"height\": height,\n",
    "                      \"polygons\": annotations}\n",
    "    \n",
    "    return img_annotation\n",
    "    \n",
    "    # with open(img_path+\".json\", 'w', encoding=\"utf-8\") as json_file:\n",
    "    #     json.dump(img_annotation, json_file, indent=4)\n",
    "        \n",
    "\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "import xml.dom.minidom\n",
    "\n",
    "def json_to_xml(json_obj):\n",
    "    # Create the root element\n",
    "    annotations = ET.Element(\"annotations\")\n",
    "    \n",
    "    # Add version element\n",
    "    version = ET.SubElement(annotations, \"version\")\n",
    "    version.text = \"1.1\"\n",
    "    \n",
    "    # Add meta element\n",
    "    meta = ET.SubElement(annotations, \"meta\")\n",
    "    job = ET.SubElement(meta, \"job\")\n",
    "    ET.SubElement(job, \"id\").text = \"12\"\n",
    "    ET.SubElement(job, \"size\").text = \"1\"\n",
    "    ET.SubElement(job, \"mode\").text = \"annotation\"\n",
    "    ET.SubElement(job, \"overlap\").text = \"0\"\n",
    "    ET.SubElement(job, \"bugtracker\").text = \"\"\n",
    "    ET.SubElement(job, \"created\").text = \"2024-04-07 14:09:14.625571+00:00\"\n",
    "    ET.SubElement(job, \"updated\").text = \"2024-08-13 20:36:33.809220+00:00\"\n",
    "    ET.SubElement(job, \"subset\").text = \"default\"\n",
    "    ET.SubElement(job, \"start_frame\").text = \"0\"\n",
    "    ET.SubElement(job, \"stop_frame\").text = \"0\"\n",
    "    ET.SubElement(job, \"frame_filter\").text = \"\"\n",
    "    \n",
    "    segments = ET.SubElement(job, \"segments\")\n",
    "    segment = ET.SubElement(segments, \"segment\")\n",
    "    ET.SubElement(segment, \"id\").text = \"12\"\n",
    "    ET.SubElement(segment, \"start\").text = \"0\"\n",
    "    ET.SubElement(segment, \"stop\").text = \"0\"\n",
    "    ET.SubElement(segment, \"url\").text = \"http://10.95.16.20:8080/api/jobs/12\"\n",
    "    \n",
    "    owner = ET.SubElement(job, \"owner\")\n",
    "    ET.SubElement(owner, \"username\").text = \"kaede.johnson\"\n",
    "    ET.SubElement(owner, \"email\").text = \"kaede.johnson@epfl.ch\"\n",
    "    \n",
    "    ET.SubElement(job, \"assignee\").text = \"\"\n",
    "    \n",
    "    labels = ET.SubElement(job, \"labels\")\n",
    "    label = ET.SubElement(labels, \"label\")\n",
    "    ET.SubElement(label, \"name\").text = \"text\"\n",
    "    ET.SubElement(label, \"color\").text = \"#fa3253\"\n",
    "    ET.SubElement(label, \"type\").text = \"polygon\"\n",
    "    \n",
    "    attributes = ET.SubElement(label, \"attributes\")\n",
    "    attribute_names = [\"illegible\", \"sequence_order\", \"transcription\", \"truncated\"]\n",
    "    for attr_name in attribute_names:\n",
    "        attribute = ET.SubElement(attributes, \"attribute\")\n",
    "        ET.SubElement(attribute, \"name\").text = attr_name\n",
    "        ET.SubElement(attribute, \"mutable\").text = \"False\"\n",
    "        input_type = \"checkbox\" if attr_name in [\"illegible\", \"truncated\"] else \"text\"\n",
    "        ET.SubElement(attribute, \"input_type\").text = input_type\n",
    "        ET.SubElement(attribute, \"default_value\").text = \"false\" if input_type == \"checkbox\" else \"\"\n",
    "        ET.SubElement(attribute, \"values\").text = \"false\" if input_type == \"checkbox\" else \"\"\n",
    "    \n",
    "    ET.SubElement(meta, \"dumped\").text = \"2024-08-14 10:10:50.238232+00:00\"\n",
    "    \n",
    "    # Create the image element\n",
    "    image = ET.SubElement(annotations, \"image\", id=\"0\", name=json_obj[\"image_name\"],\n",
    "                          width=str(json_obj[\"width\"]), height=str(json_obj[\"height\"]))\n",
    "    \n",
    "    # Create polygon elements based on the JSON polygons\n",
    "    for polygon in json_obj[\"polygons\"]:\n",
    "        polygon_element = ET.SubElement(image, \"polygon\", label=\"text\", source=\"manual\", occluded=\"0\",\n",
    "                                        points=polygon[\"points\"], z_order=str(polygon[\"z_order\"]),\n",
    "                                        group_id=str(polygon[\"group_id\"]))\n",
    "        \n",
    "        for attr in [\"transcription\", \"illegible\", \"truncated\", \"sequence_order\"]:\n",
    "            attribute_element = ET.SubElement(polygon_element, \"attribute\", name=attr)\n",
    "            if attr == \"sequence_order\":\n",
    "                if str(polygon[attr]) == \"None\":\n",
    "                    attribute_element.text = \"\"\n",
    "                else:\n",
    "                    attribute_element.text = str(polygon[attr])\n",
    "            else:\n",
    "                attribute_element.text = str(polygon[attr])\n",
    "    \n",
    "    # Convert the ElementTree to a string\n",
    "    xml_str = ET.tostring(annotations, encoding='unicode', method='xml')\n",
    "    \n",
    "    return xml_str\n",
    "\n",
    "# df = pd.read_pickle(\"./new_annotation/maps_res/catherwood_1835/catherwood_1835.jpeg.pkl\")\n",
    "# json_data = convert_pkl_to_json(df)\n",
    "# print(json_data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-04T12:20:28.604900700Z",
     "start_time": "2024-09-04T12:20:28.564774800Z"
    }
   },
   "id": "2fff0d4858b537de",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for crop in df_annotation[\"crop_name\"].unique().tolist():\n",
    "    \n",
    "    try:\n",
    "        print(\"Converting {} to xml\".format(crop))\n",
    "    \n",
    "        file_path = find_file_recursive(\"./new_annotation/maps_res/\", crop)\n",
    "        dir_path = os.path.dirname(file_path)\n",
    "        df = pd.read_pickle(dir_path + \"/\" +  crop+ \".pkl\")\n",
    "    \n",
    "        json_data = convert_pkl_to_json(df)\n",
    "        xml_str = json_to_xml(json_data)\n",
    "        dom = xml.dom.minidom.parseString(xml_str)\n",
    "        pretty_xml_as_string = dom.toprettyxml()\n",
    "    \n",
    "        with open(dir_path + \"/\" +  crop+ \".xml\", \"w\", encoding=\"utf-8\") as xml_file:\n",
    "            xml_file.write(pretty_xml_as_string)\n",
    "    \n",
    "    except:\n",
    "        print(\"Failed converting {}\".format(crop))\n",
    "        continue"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "30317829c25334b3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def xml_to_df(crop_name):\n",
    "\n",
    "    tree = ET.parse('./new_annotation/maps_res/{}/annotations.xml'.format(crop_name.split(\".\")[0]))\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    # for each crop annotated\n",
    "    for image in root.findall('image'):\n",
    "    \n",
    "        # access all polygons (aka words)\n",
    "        polygons = image.findall('.//polygon')\n",
    "        polygon_list = []\n",
    "        for polygon in polygons:\n",
    "            tmp_dict = {}\n",
    "            tmp_dict['vertices'] = [tuple(map(float, pair.split(','))) for pair in polygon.get('points').split(';')]\n",
    "            #print(polygon.find(\".//attribute[@name='sequence_order']\").text)\n",
    "            if polygon.find(\".//attribute[@name='sequence_order']\").text is not None:\n",
    "                tmp_dict['group_id'] = int(polygon.get('group_id'))\n",
    "            else:\n",
    "                tmp_dict['group_id'] = -1    \n",
    "            for attribute in polygon.findall('.//attribute'):\n",
    "                if attribute.get('name') == 'transcription':\n",
    "                    tmp_dict['text'] = attribute.text\n",
    "                else:\n",
    "                    tmp_dict[attribute.get('name')] = attribute.text\n",
    "            polygon_list.append(tmp_dict)\n",
    "        polygon_df = pd.DataFrame(polygon_list).sort_values(by=['group_id', 'sequence_order']).reindex(columns=['vertices', 'text', 'illegible', 'truncated', 'group_id', 'sequence_order'])\n",
    "    \n",
    "        return polygon_df\n",
    "    \n",
    "\n",
    "def plot_and_save_ultimate(img_name, df):\n",
    "    \n",
    "    file_path = find_file_recursive(\"./new_annotation/maps_res/\", img_name)\n",
    "    dir_path = os.path.dirname(file_path)\n",
    "    \n",
    "    image = Image.open(file_path)\n",
    "    \n",
    "    draw = ImageDraw.Draw(image)\n",
    "    \n",
    "    corrected_vertices = df[df[\"crop_name\"]==img_name][\"vertices_final\"].tolist()\n",
    "    texts = df[df[\"crop_name\"]==img_name][\"text_final\"].tolist()\n",
    "    for vertices, text in zip(corrected_vertices, texts):\n",
    "        try:\n",
    "            draw.line(vertices[0] + vertices[1], fill='orange', width=2)\n",
    "            draw.line(vertices[0], fill=\"blue\", width=2)\n",
    "            draw.line(vertices[1], fill=\"blue\", width=2)\n",
    "            draw.text(vertices[1][-1], text=text)\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "    # image.show()\n",
    "    image.save(dir_path + \"/\" + img_name.split('.')[0]+\"_ultimate_corr.png\")\n",
    "    \n",
    "    df.to_pickle(dir_path + \"/\" + img_name.split('.')[0]+\"_ultimate_corr.png.pkl\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-04T12:20:33.513540300Z",
     "start_time": "2024-09-04T12:20:33.498515600Z"
    }
   },
   "id": "9087ed63fad43a93",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "148it [00:01, 92.15it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148\n",
      "148\n",
      "set()\n",
      "set()\n",
      "set()\n",
      "set()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "filename = \"unknown_1907_crop_2468_1674.jpg\"\n",
    "df_new = xml_to_df(filename)\n",
    "df_old = pd.read_pickle(find_file_recursive(\"new_annotation/maps_res\", \"{}.pkl\".format(filename)))\n",
    "\n",
    "old_polygons = df_old[\"vertices\"].tolist()\n",
    "new_polygons = df_new[\"vertices\"].tolist()\n",
    "\n",
    "iou_threshold = 0.1\n",
    "matches = match_polygons(old_polygons, new_polygons, iou_threshold)\n",
    "\n",
    "print(len(matches))\n",
    "print(len(df_old))\n",
    "\n",
    "print(set(range(len(df_old))) - set([match[0] for match in matches]))\n",
    "print(set(range(len(df_old))) - set([match[1] for match in matches]))\n",
    "print(set([match[0] for match in matches]) - set([match[0] for match in matches]).intersection(set([match[1] for match in matches])))\n",
    "print(set([match[1] for match in matches]) - set([match[0] for match in matches]).intersection(set([match[1] for match in matches])))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-04T12:24:11.274905100Z",
     "start_time": "2024-09-04T12:24:09.609903900Z"
    }
   },
   "id": "526f9c5b6b41087c",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_matches = None\n",
    "for match in matches:\n",
    "    df_matches = pd.concat((df_matches, pd.DataFrame({\"vertices\": [df_old.iloc[match[0]][\"vertices\"]],\n",
    "                                                     \"vertices_new\": [df_new.iloc[match[1]][\"vertices\"]],\n",
    "                                                      \"text_new\": [df_new.iloc[match[1]][\"text\"]]})))\n",
    "\n",
    "df_old['vertices'] = df_old['vertices'].apply(str)\n",
    "df_matches['vertices'] = df_matches['vertices'].apply(str)\n",
    "df_merged = pd.merge(df_old, df_matches, on='vertices', how='inner')\n",
    "df_merged['vertices'] = df_merged['vertices'].apply(ast.literal_eval)\n",
    "df_merged['vertices'] = df_merged['vertices'].apply(lambda x: [(point[0], point[1]) for point in x])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-04T12:24:13.392757600Z",
     "start_time": "2024-09-04T12:24:13.268782500Z"
    }
   },
   "id": "951bc95dd5c874b1",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "                                              vertices      text illegible  \\\n0    [(147.41, 211.41), (143.81, 211.16), (140.6, 2...         5     false   \n1    [(91.3, 589.41), (84.18, 589.38), (77.19, 589....     Tower     false   \n2    [(151.78, 586.46), (148.2, 586.41), (144.94, 5...         6     false   \n3    [(360.1, 799.08), (356.28, 799.16), (352.04, 7...         C     false   \n4    [(905.69, 83.16), (894.32, 83.48), (883.21, 83...  Hospital     false   \n..                                                 ...       ...       ...   \n143  [(840.49, 522.23), (858.24, 528.74), (853.21, ...        az     false   \n144  [(794.33, 526.67), (845.81, 545.9), (843.45, 5...   ZEITUNE     false   \n145  [(592.01, 414.55), (592.23, 412.73), (593.2, 4...     HÂRAT     false   \n146  [(575.32, 519.34), (575.32, 519.34), (575.32, ...        al     false   \n147  [(574.48, 618.68), (575.36, 629.81), (576.22, ...     ARMEN     false   \n\n    truncated  group_id sequence_order                        crop_name  \\\n0       false        -1           None  unknown_1907_crop_2468_1674.jpg   \n1       false        -1           None  unknown_1907_crop_2468_1674.jpg   \n2       false        -1           None  unknown_1907_crop_2468_1674.jpg   \n3       false        -1           None  unknown_1907_crop_2468_1674.jpg   \n4       false        -1           None  unknown_1907_crop_2468_1674.jpg   \n..        ...       ...            ...                              ...   \n143     false        49              2  unknown_1907_crop_2468_1674.jpg   \n144     false        49              3  unknown_1907_crop_2468_1674.jpg   \n145     false        50              1  unknown_1907_crop_2468_1674.jpg   \n146     false        50              2  unknown_1907_crop_2468_1674.jpg   \n147     false        50              3  unknown_1907_crop_2468_1674.jpg   \n\n                                    vertices_corrected  OCR_text  \\\n0    [(123.0235900879, 215.4983062744), (123.738998...         5   \n1    [(41.5743255615, 589.8125457764), (43.78880310...     Tower   \n2    [(127.4473495483, 588.6155090332), (127.814918...         6   \n3    [(330.2155838013, 803.9237213135), (331.135627...         C   \n4    [(828.5953674316, 84.6456375122), (832.4320983...  Hospital   \n..                                                 ...       ...   \n143                                                NaN       NaN   \n144  [(792.5183410645, 525.6031265259), (794.742759...   TEITUNE   \n145  [(591.3165283203, 388.0397338867), (591.556427...     HARAT   \n146                                                NaN       NaN   \n147  [(587.6629180908, 606.9669189453), (587.765945...     ERMEN   \n\n                                   vertices_simplified  \\\n0    [[(127.2040710449, 245.2281951904), (127.96829...   \n1    [[(91.3, 589.41), (42.86, 589.29)], [(42.55, 6...   \n2    [[(130.5075378418, 616.1682281494), (130.69633...   \n3    [[(338.8154830933, 833.8014678955), (339.95793...   \n4    [[(828.54, 102.95), (905.22, 102.01)], [(905.6...   \n..                                                 ...   \n143  [[(858.24, 528.74), (840.49, 522.23)], [(835.1...   \n144  [[(845.81, 545.9), (794.33, 526.67)], [(789.3,...   \n145  [[(590.93, 467.69), (592.01, 414.55), (592.32,...   \n146  [[(575.32, 519.34), (574.85, 549.42)], [(589.4...   \n147  [[(592.16, 684.71), (587.69, 606.31)], [(573.6...   \n\n                                               feature  \\\n0    [18.441848754899993, -33.0678405761, -26.06521...   \n1    [-48.75, 18.379999999999995, 48.59, 18.4100000...   \n2    [18.366958618199988, -30.91835021969996, -24.4...   \n3    [17.047203063900042, -39.3532791138, -35.49494...   \n4    [77.15000000000009, -19.790000000000006, -76.3...   \n..                                                 ...   \n143  [-23.08000000000004, 4.439999999999941, 12.720...   \n144  [-56.50999999999999, -8.579999999999927, 49.12...   \n145  [-13.469999999999914, -81.99000000000001, -16....   \n146  [14.120000000000005, 28.91999999999996, 13.370...   \n147  [-18.519999999999982, -78.86000000000001, -9.2...   \n\n                   vertices_simplified_order_corrected  \\\n0    [[(127.2040710449, 245.2281951904), (127.96829...   \n1    [[(42.55, 607.79), (91.45, 607.7)], [(91.3, 58...   \n2    [[(130.5075378418, 616.1682281494), (130.69633...   \n3    [[(338.8154830933, 833.8014678955), (339.95793...   \n4    [[(828.54, 102.95), (905.22, 102.01)], [(905.6...   \n..                                                 ...   \n143  [[(835.16, 533.18), (853.21, 539.39)], [(858.2...   \n144  [[(789.3, 537.32), (843.45, 555.96)], [(845.81...   \n145  [[(577.46, 385.7), (576.29, 468.32)], [(590.93...   \n146  [[(575.32, 519.34), (574.85, 549.42)], [(589.4...   \n147  [[(573.64, 605.85), (574.48, 618.68), (578.45,...   \n\n                                          vertices_new  text_new  \\\n0    [(123.02, 215.5), (123.74, 215.36), (124.7, 21...         5   \n1    [(42.86, 589.29), (91.3, 589.41), (91.45, 607....     Tower   \n2    [(128.12, 588.13), (149.9, 588.77), (149.9, 61...        6/   \n3    [(330.22, 803.92), (331.14, 803.5), (332.28, 8...         C   \n4    [(828.86, 84.52), (905.69, 83.16), (905.22, 10...  Hospital   \n..                                                 ...       ...   \n143  [(840.49, 522.23), (858.24, 528.74), (853.21, ...        AZ   \n144  [(794.33, 526.67), (845.81, 545.9), (843.45, 5...   ZEITUNE   \n145  [(593.58, 387.92), (593.58, 465.95), (574.36, ...    HÂRAT/   \n146  [(588.22, 517.17), (589.44, 548.26), (574.85, ...        AL   \n147  [(587.69, 606.31), (592.16, 684.71), (578.45, ...     ARMEN   \n\n                                        vertices_final text_final  \n0    [[(123.0235900879, 215.4983062744), (123.73899...          5  \n1    [[(42.86, 589.29), (91.3, 589.41)], [(91.45, 6...      Tower  \n2    [[(128.12, 588.13), (149.9, 588.77)], [(149.9,...          6  \n3    [[(330.2155838013, 803.9237213135), (331.13562...          C  \n4    [[(828.86, 84.52), (905.69, 83.16)], [(905.22,...   Hospital  \n..                                                 ...        ...  \n143  [[(840.49, 522.23), (858.24, 528.74)], [(853.2...         AZ  \n144  [[(794.33, 526.67), (845.81, 545.9)], [(843.45...    ZEITUNE  \n145  [[(593.58, 387.92), (593.58, 465.95)], [(574.3...      HÂRAT  \n146  [[(588.22, 517.17), (589.44, 548.26)], [(574.8...         AL  \n147  [[(587.69, 606.31), (592.16, 684.71)], [(578.4...      ARMEN  \n\n[148 rows x 16 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>vertices</th>\n      <th>text</th>\n      <th>illegible</th>\n      <th>truncated</th>\n      <th>group_id</th>\n      <th>sequence_order</th>\n      <th>crop_name</th>\n      <th>vertices_corrected</th>\n      <th>OCR_text</th>\n      <th>vertices_simplified</th>\n      <th>feature</th>\n      <th>vertices_simplified_order_corrected</th>\n      <th>vertices_new</th>\n      <th>text_new</th>\n      <th>vertices_final</th>\n      <th>text_final</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[(147.41, 211.41), (143.81, 211.16), (140.6, 2...</td>\n      <td>5</td>\n      <td>false</td>\n      <td>false</td>\n      <td>-1</td>\n      <td>None</td>\n      <td>unknown_1907_crop_2468_1674.jpg</td>\n      <td>[(123.0235900879, 215.4983062744), (123.738998...</td>\n      <td>5</td>\n      <td>[[(127.2040710449, 245.2281951904), (127.96829...</td>\n      <td>[18.441848754899993, -33.0678405761, -26.06521...</td>\n      <td>[[(127.2040710449, 245.2281951904), (127.96829...</td>\n      <td>[(123.02, 215.5), (123.74, 215.36), (124.7, 21...</td>\n      <td>5</td>\n      <td>[[(123.0235900879, 215.4983062744), (123.73899...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[(91.3, 589.41), (84.18, 589.38), (77.19, 589....</td>\n      <td>Tower</td>\n      <td>false</td>\n      <td>false</td>\n      <td>-1</td>\n      <td>None</td>\n      <td>unknown_1907_crop_2468_1674.jpg</td>\n      <td>[(41.5743255615, 589.8125457764), (43.78880310...</td>\n      <td>Tower</td>\n      <td>[[(91.3, 589.41), (42.86, 589.29)], [(42.55, 6...</td>\n      <td>[-48.75, 18.379999999999995, 48.59, 18.4100000...</td>\n      <td>[[(42.55, 607.79), (91.45, 607.7)], [(91.3, 58...</td>\n      <td>[(42.86, 589.29), (91.3, 589.41), (91.45, 607....</td>\n      <td>Tower</td>\n      <td>[[(42.86, 589.29), (91.3, 589.41)], [(91.45, 6...</td>\n      <td>Tower</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[(151.78, 586.46), (148.2, 586.41), (144.94, 5...</td>\n      <td>6</td>\n      <td>false</td>\n      <td>false</td>\n      <td>-1</td>\n      <td>None</td>\n      <td>unknown_1907_crop_2468_1674.jpg</td>\n      <td>[(127.4473495483, 588.6155090332), (127.814918...</td>\n      <td>6</td>\n      <td>[[(130.5075378418, 616.1682281494), (130.69633...</td>\n      <td>[18.366958618199988, -30.91835021969996, -24.4...</td>\n      <td>[[(130.5075378418, 616.1682281494), (130.69633...</td>\n      <td>[(128.12, 588.13), (149.9, 588.77), (149.9, 61...</td>\n      <td>6/</td>\n      <td>[[(128.12, 588.13), (149.9, 588.77)], [(149.9,...</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[(360.1, 799.08), (356.28, 799.16), (352.04, 7...</td>\n      <td>C</td>\n      <td>false</td>\n      <td>false</td>\n      <td>-1</td>\n      <td>None</td>\n      <td>unknown_1907_crop_2468_1674.jpg</td>\n      <td>[(330.2155838013, 803.9237213135), (331.135627...</td>\n      <td>C</td>\n      <td>[[(338.8154830933, 833.8014678955), (339.95793...</td>\n      <td>[17.047203063900042, -39.3532791138, -35.49494...</td>\n      <td>[[(338.8154830933, 833.8014678955), (339.95793...</td>\n      <td>[(330.22, 803.92), (331.14, 803.5), (332.28, 8...</td>\n      <td>C</td>\n      <td>[[(330.2155838013, 803.9237213135), (331.13562...</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[(905.69, 83.16), (894.32, 83.48), (883.21, 83...</td>\n      <td>Hospital</td>\n      <td>false</td>\n      <td>false</td>\n      <td>-1</td>\n      <td>None</td>\n      <td>unknown_1907_crop_2468_1674.jpg</td>\n      <td>[(828.5953674316, 84.6456375122), (832.4320983...</td>\n      <td>Hospital</td>\n      <td>[[(828.54, 102.95), (905.22, 102.01)], [(905.6...</td>\n      <td>[77.15000000000009, -19.790000000000006, -76.3...</td>\n      <td>[[(828.54, 102.95), (905.22, 102.01)], [(905.6...</td>\n      <td>[(828.86, 84.52), (905.69, 83.16), (905.22, 10...</td>\n      <td>Hospital</td>\n      <td>[[(828.86, 84.52), (905.69, 83.16)], [(905.22,...</td>\n      <td>Hospital</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>143</th>\n      <td>[(840.49, 522.23), (858.24, 528.74), (853.21, ...</td>\n      <td>az</td>\n      <td>false</td>\n      <td>false</td>\n      <td>49</td>\n      <td>2</td>\n      <td>unknown_1907_crop_2468_1674.jpg</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>[[(858.24, 528.74), (840.49, 522.23)], [(835.1...</td>\n      <td>[-23.08000000000004, 4.439999999999941, 12.720...</td>\n      <td>[[(835.16, 533.18), (853.21, 539.39)], [(858.2...</td>\n      <td>[(840.49, 522.23), (858.24, 528.74), (853.21, ...</td>\n      <td>AZ</td>\n      <td>[[(840.49, 522.23), (858.24, 528.74)], [(853.2...</td>\n      <td>AZ</td>\n    </tr>\n    <tr>\n      <th>144</th>\n      <td>[(794.33, 526.67), (845.81, 545.9), (843.45, 5...</td>\n      <td>ZEITUNE</td>\n      <td>false</td>\n      <td>false</td>\n      <td>49</td>\n      <td>3</td>\n      <td>unknown_1907_crop_2468_1674.jpg</td>\n      <td>[(792.5183410645, 525.6031265259), (794.742759...</td>\n      <td>TEITUNE</td>\n      <td>[[(845.81, 545.9), (794.33, 526.67)], [(789.3,...</td>\n      <td>[-56.50999999999999, -8.579999999999927, 49.12...</td>\n      <td>[[(789.3, 537.32), (843.45, 555.96)], [(845.81...</td>\n      <td>[(794.33, 526.67), (845.81, 545.9), (843.45, 5...</td>\n      <td>ZEITUNE</td>\n      <td>[[(794.33, 526.67), (845.81, 545.9)], [(843.45...</td>\n      <td>ZEITUNE</td>\n    </tr>\n    <tr>\n      <th>145</th>\n      <td>[(592.01, 414.55), (592.23, 412.73), (593.2, 4...</td>\n      <td>HÂRAT</td>\n      <td>false</td>\n      <td>false</td>\n      <td>50</td>\n      <td>1</td>\n      <td>unknown_1907_crop_2468_1674.jpg</td>\n      <td>[(591.3165283203, 388.0397338867), (591.556427...</td>\n      <td>HARAT</td>\n      <td>[[(590.93, 467.69), (592.01, 414.55), (592.32,...</td>\n      <td>[-13.469999999999914, -81.99000000000001, -16....</td>\n      <td>[[(577.46, 385.7), (576.29, 468.32)], [(590.93...</td>\n      <td>[(593.58, 387.92), (593.58, 465.95), (574.36, ...</td>\n      <td>HÂRAT/</td>\n      <td>[[(593.58, 387.92), (593.58, 465.95)], [(574.3...</td>\n      <td>HÂRAT</td>\n    </tr>\n    <tr>\n      <th>146</th>\n      <td>[(575.32, 519.34), (575.32, 519.34), (575.32, ...</td>\n      <td>al</td>\n      <td>false</td>\n      <td>false</td>\n      <td>50</td>\n      <td>2</td>\n      <td>unknown_1907_crop_2468_1674.jpg</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>[[(575.32, 519.34), (574.85, 549.42)], [(589.4...</td>\n      <td>[14.120000000000005, 28.91999999999996, 13.370...</td>\n      <td>[[(575.32, 519.34), (574.85, 549.42)], [(589.4...</td>\n      <td>[(588.22, 517.17), (589.44, 548.26), (574.85, ...</td>\n      <td>AL</td>\n      <td>[[(588.22, 517.17), (589.44, 548.26)], [(574.8...</td>\n      <td>AL</td>\n    </tr>\n    <tr>\n      <th>147</th>\n      <td>[(574.48, 618.68), (575.36, 629.81), (576.22, ...</td>\n      <td>ARMEN</td>\n      <td>false</td>\n      <td>false</td>\n      <td>50</td>\n      <td>3</td>\n      <td>unknown_1907_crop_2468_1674.jpg</td>\n      <td>[(587.6629180908, 606.9669189453), (587.765945...</td>\n      <td>ERMEN</td>\n      <td>[[(592.16, 684.71), (587.69, 606.31)], [(573.6...</td>\n      <td>[-18.519999999999982, -78.86000000000001, -9.2...</td>\n      <td>[[(573.64, 605.85), (574.48, 618.68), (578.45,...</td>\n      <td>[(587.69, 606.31), (592.16, 684.71), (578.45, ...</td>\n      <td>ARMEN</td>\n      <td>[[(587.69, 606.31), (592.16, 684.71)], [(578.4...</td>\n      <td>ARMEN</td>\n    </tr>\n  </tbody>\n</table>\n<p>148 rows × 16 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_final_vertices(x):\n",
    "    # if x[\"vertices_simplified_order_corrected\"] is None:\n",
    "    #     print(x[\"text_new\"])\n",
    "    #     print(x[\"vertices_new\"])\n",
    "    if x[\"text_new\"].endswith(\"/\") and not x[\"text_new\"].endswith(\"//\"):\n",
    "        n_vertices = len(x[\"vertices_new\"])\n",
    "        # assert n_vertices % 2 == 0\n",
    "        if n_vertices % 2 != 0:\n",
    "            print(x[\"text_new\"])\n",
    "            print(x[\"vertices\"])\n",
    "        final_vertices = [x[\"vertices_new\"][:n_vertices//2], x[\"vertices_new\"][n_vertices//2:]]\n",
    "        # assert is_clockwise(Polygon(final_vertices[0] + final_vertices[1]))\n",
    "        return final_vertices\n",
    "    elif x[\"text_new\"].endswith(\"//\"):\n",
    "        final_vertices = [x[\"vertices_simplified_order_corrected\"][0][::-1],\n",
    "                          x[\"vertices_simplified_order_corrected\"][1][::-1]]\n",
    "        # assert is_clockwise(Polygon(final_vertices[0] + final_vertices[1]))\n",
    "        return final_vertices\n",
    "    else:\n",
    "        final_vertices = [x[\"vertices_simplified_order_corrected\"][1][::-1],\n",
    "                          x[\"vertices_simplified_order_corrected\"][0][::-1]]\n",
    "        # assert is_clockwise(Polygon(final_vertices[0] + final_vertices[1]))\n",
    "        return final_vertices\n",
    "\n",
    "df_merged[\"vertices_final\"] = df_merged.apply(get_final_vertices, axis=1)\n",
    "df_merged[\"text_final\"] = df_merged.apply(lambda x: x[\"text_new\"].strip(\"/\"), axis=1)\n",
    "display(df_merged)\n",
    "plot_and_save_ultimate(filename, df_merged)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-04T12:24:16.397541400Z",
     "start_time": "2024-09-04T12:24:14.937539600Z"
    }
   },
   "id": "b6576cdb73575072",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "                                             vertices    text illegible  \\\n23  [(912.0, 429.08), (911.44, 439.28), (912.11, 4...  (MOUNT     false   \n24  [(570.84, 312.72), (571.41, 304.97), (571.78, ...   hârat     false   \n25  [(914.84, 366.93), (914.6, 374.04), (913.93, 3...    SION     false   \n26  [(560.58, 252.28), (560.62, 255.11), (560.68, ...      al     false   \n27  [(920.47, 312.9), (919.55, 319.38), (918.74, 3...   STR.)     false   \n28  [(560.87, 211.8), (560.9, 218.04), (560.53, 22...    kalâ     false   \n\n   truncated  group_id sequence_order                        crop_name  \\\n23     false         1              1  unknown_1907_crop_2468_1674.jpg   \n24     false         1              1  unknown_1907_crop_2468_1674.jpg   \n25     false         1              2  unknown_1907_crop_2468_1674.jpg   \n26     false         1              2  unknown_1907_crop_2468_1674.jpg   \n27     false         1              3  unknown_1907_crop_2468_1674.jpg   \n28     false         1              3  unknown_1907_crop_2468_1674.jpg   \n\n                                   vertices_corrected OCR_text  \\\n23  [(914.6887054443, 494.7551879883), (914.978088...    NOUET   \n24                                                NaN      NaN   \n25  [(911.770904541, 414.077835083), (911.95385742...      SON   \n26                                                NaN      NaN   \n27  [(916.1359100342, 358.9216003418), (916.401794...     STR.   \n28                                                NaN      NaN   \n\n                                  vertices_simplified  \\\n23  [[(914.1, 421.8), (912.0, 429.08), (913.7, 490...   \n24  [[(570.84, 312.72), (573.75, 260.99)], [(559.8...   \n25  [[(926.7, 415.45), (928.56, 368.61)], [(914.84...   \n26  [[(573.55, 258.11), (573.12, 238.45)], [(560.6...   \n27  [[(929.85, 360.41), (935.66, 316.17)], [(920.4...   \n28  [[(574.21, 236.37), (574.35, 193.65)], [(560.1...   \n\n                                              feature  \\\n23  [13.399999999999977, 73.59999999999997, 12.829...   \n24  [-10.970000000000027, -52.870000000000005, -16...   \n25  [-11.860000000000014, -48.51999999999998, -15....   \n26  [-12.919999999999959, -20.580000000000013, -12...   \n27  [-9.379999999999995, -47.51000000000005, -20.1...   \n28  [-14.06000000000006, -43.06999999999999, -13.7...   \n\n                  vertices_simplified_order_corrected  \\\n23  [[(914.1, 421.8), (912.0, 429.08), (913.7, 490...   \n24  [[(559.87, 259.85), (557.22, 311.62)], [(570.8...   \n25  [[(914.84, 366.93), (913.03, 414.89)], [(926.7...   \n26  [[(560.63, 237.53), (560.58, 252.28), (560.78,...   \n27  [[(920.47, 312.9), (915.5, 358.51)], [(929.85,...   \n28  [[(560.15, 193.3), (560.87, 211.8), (560.58, 2...   \n\n                                         vertices_new  text_new  \\\n23  [(926.53, 423.34), (927.5, 495.4), (913.7, 490...  (MOUNT//   \n24  [(573.75, 260.99), (570.84, 312.72), (557.22, ...   HÂRAT//   \n25  [(928.56, 368.61), (926.7, 415.45), (913.03, 4...    SION//   \n26  [(573.12, 238.45), (573.55, 258.11), (560.78, ...      AL//   \n27  [(935.66, 316.17), (929.85, 360.41), (915.5, 3...   STR.)//   \n28  [(574.35, 193.65), (574.21, 236.37), (560.58, ...    KALÂ//   \n\n                                       vertices_final text_final  \n23  [[(913.7, 490.0), (912.0, 429.08), (914.1, 421...     (MOUNT  \n24  [[(557.22, 311.62), (559.87, 259.85)], [(573.7...      HÂRAT  \n25  [[(913.03, 414.89), (914.84, 366.93)], [(928.5...       SION  \n26  [[(560.78, 258.47), (560.58, 252.28), (560.63,...         AL  \n27  [[(915.5, 358.51), (920.47, 312.9)], [(935.66,...      STR.)  \n28  [[(560.58, 237.68), (560.87, 211.8), (560.15, ...       KALÂ  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>vertices</th>\n      <th>text</th>\n      <th>illegible</th>\n      <th>truncated</th>\n      <th>group_id</th>\n      <th>sequence_order</th>\n      <th>crop_name</th>\n      <th>vertices_corrected</th>\n      <th>OCR_text</th>\n      <th>vertices_simplified</th>\n      <th>feature</th>\n      <th>vertices_simplified_order_corrected</th>\n      <th>vertices_new</th>\n      <th>text_new</th>\n      <th>vertices_final</th>\n      <th>text_final</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>23</th>\n      <td>[(912.0, 429.08), (911.44, 439.28), (912.11, 4...</td>\n      <td>(MOUNT</td>\n      <td>false</td>\n      <td>false</td>\n      <td>1</td>\n      <td>1</td>\n      <td>unknown_1907_crop_2468_1674.jpg</td>\n      <td>[(914.6887054443, 494.7551879883), (914.978088...</td>\n      <td>NOUET</td>\n      <td>[[(914.1, 421.8), (912.0, 429.08), (913.7, 490...</td>\n      <td>[13.399999999999977, 73.59999999999997, 12.829...</td>\n      <td>[[(914.1, 421.8), (912.0, 429.08), (913.7, 490...</td>\n      <td>[(926.53, 423.34), (927.5, 495.4), (913.7, 490...</td>\n      <td>(MOUNT//</td>\n      <td>[[(913.7, 490.0), (912.0, 429.08), (914.1, 421...</td>\n      <td>(MOUNT</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>[(570.84, 312.72), (571.41, 304.97), (571.78, ...</td>\n      <td>hârat</td>\n      <td>false</td>\n      <td>false</td>\n      <td>1</td>\n      <td>1</td>\n      <td>unknown_1907_crop_2468_1674.jpg</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>[[(570.84, 312.72), (573.75, 260.99)], [(559.8...</td>\n      <td>[-10.970000000000027, -52.870000000000005, -16...</td>\n      <td>[[(559.87, 259.85), (557.22, 311.62)], [(570.8...</td>\n      <td>[(573.75, 260.99), (570.84, 312.72), (557.22, ...</td>\n      <td>HÂRAT//</td>\n      <td>[[(557.22, 311.62), (559.87, 259.85)], [(573.7...</td>\n      <td>HÂRAT</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>[(914.84, 366.93), (914.6, 374.04), (913.93, 3...</td>\n      <td>SION</td>\n      <td>false</td>\n      <td>false</td>\n      <td>1</td>\n      <td>2</td>\n      <td>unknown_1907_crop_2468_1674.jpg</td>\n      <td>[(911.770904541, 414.077835083), (911.95385742...</td>\n      <td>SON</td>\n      <td>[[(926.7, 415.45), (928.56, 368.61)], [(914.84...</td>\n      <td>[-11.860000000000014, -48.51999999999998, -15....</td>\n      <td>[[(914.84, 366.93), (913.03, 414.89)], [(926.7...</td>\n      <td>[(928.56, 368.61), (926.7, 415.45), (913.03, 4...</td>\n      <td>SION//</td>\n      <td>[[(913.03, 414.89), (914.84, 366.93)], [(928.5...</td>\n      <td>SION</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>[(560.58, 252.28), (560.62, 255.11), (560.68, ...</td>\n      <td>al</td>\n      <td>false</td>\n      <td>false</td>\n      <td>1</td>\n      <td>2</td>\n      <td>unknown_1907_crop_2468_1674.jpg</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>[[(573.55, 258.11), (573.12, 238.45)], [(560.6...</td>\n      <td>[-12.919999999999959, -20.580000000000013, -12...</td>\n      <td>[[(560.63, 237.53), (560.58, 252.28), (560.78,...</td>\n      <td>[(573.12, 238.45), (573.55, 258.11), (560.78, ...</td>\n      <td>AL//</td>\n      <td>[[(560.78, 258.47), (560.58, 252.28), (560.63,...</td>\n      <td>AL</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>[(920.47, 312.9), (919.55, 319.38), (918.74, 3...</td>\n      <td>STR.)</td>\n      <td>false</td>\n      <td>false</td>\n      <td>1</td>\n      <td>3</td>\n      <td>unknown_1907_crop_2468_1674.jpg</td>\n      <td>[(916.1359100342, 358.9216003418), (916.401794...</td>\n      <td>STR.</td>\n      <td>[[(929.85, 360.41), (935.66, 316.17)], [(920.4...</td>\n      <td>[-9.379999999999995, -47.51000000000005, -20.1...</td>\n      <td>[[(920.47, 312.9), (915.5, 358.51)], [(929.85,...</td>\n      <td>[(935.66, 316.17), (929.85, 360.41), (915.5, 3...</td>\n      <td>STR.)//</td>\n      <td>[[(915.5, 358.51), (920.47, 312.9)], [(935.66,...</td>\n      <td>STR.)</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>[(560.87, 211.8), (560.9, 218.04), (560.53, 22...</td>\n      <td>kalâ</td>\n      <td>false</td>\n      <td>false</td>\n      <td>1</td>\n      <td>3</td>\n      <td>unknown_1907_crop_2468_1674.jpg</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>[[(574.21, 236.37), (574.35, 193.65)], [(560.1...</td>\n      <td>[-14.06000000000006, -43.06999999999999, -13.7...</td>\n      <td>[[(560.15, 193.3), (560.87, 211.8), (560.58, 2...</td>\n      <td>[(574.35, 193.65), (574.21, 236.37), (560.58, ...</td>\n      <td>KALÂ//</td>\n      <td>[[(560.58, 237.68), (560.87, 211.8), (560.15, ...</td>\n      <td>KALÂ</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged[df_merged[\"group_id\"]==1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-04T12:25:20.337212900Z",
     "start_time": "2024-09-04T12:25:20.178216900Z"
    }
   },
   "id": "4eafd60fab64a9cc",
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
